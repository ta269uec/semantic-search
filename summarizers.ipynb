{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "2pivMxZmcVQs"
      },
      "id": "2pivMxZmcVQs"
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "4qhwJ0ARrE3O",
        "outputId": "17a64344-3c56-4635-f1d8-f5b82f40828a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4qhwJ0ARrE3O",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 30 00:59:51 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -U sentencepiece accelerate -q\n",
        "!pip install bitsandbytes -q"
      ],
      "metadata": {
        "id": "8wpHna5dqOMv",
        "outputId": "72e41d87-ee9f-4a52-9522-f7e123df565b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "8wpHna5dqOMv",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.utils.logging import set_verbosity\n",
        "\n",
        "set_verbosity(40)\n",
        "\n",
        "import warnings\n",
        "# ignore hf pipeline complaints\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers')\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='transformers')"
      ],
      "metadata": {
        "id": "7c3rfe37qsKR"
      },
      "id": "7c3rfe37qsKR",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "gRZIdG12cof9"
      },
      "id": "gRZIdG12cof9",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarizer"
      ],
      "metadata": {
        "id": "SmDAf4rKwD5b"
      },
      "id": "SmDAf4rKwD5b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use an 8-bit quantized version of the pszemrajlong-t5-tglobal-xl-16384-book-summary model, The model has been compressed using bitsandbytes and can be loaded with low memory usage.\n",
        "\n",
        "[Details](https://huggingface.co/pszemraj/long-t5-tglobal-xl-16384-book-summary-8bit)\n"
      ],
      "metadata": {
        "id": "hcfEyJZdwNmm"
      },
      "id": "hcfEyJZdwNmm"
    },
    {
      "cell_type": "code",
      "source": [
        "class Summarizer():\n",
        "  def __init__(self):\n",
        "    hf_tag = \"pszemraj/long-t5-tglobal-xl-16384-book-summary-8bit\"\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(hf_tag)\n",
        "    self.model = AutoModelForSeq2SeqLM.from_pretrained(hf_tag, load_in_8bit=True, device_map=\"auto\")\n",
        "    self.params = {\n",
        "                      \"max_length\": 256,\n",
        "                      \"min_length\": 8,\n",
        "                      \"no_repeat_ngram_size\": 3,\n",
        "                      \"early_stopping\": True,\n",
        "                      \"repetition_penalty\": 3.5,\n",
        "                      \"length_penalty\": 0.4,\n",
        "                      \"encoder_no_repeat_ngram_size\": 3,\n",
        "                      \"num_beams\": 4,\n",
        "                   } # parameters for text generation out of model\n",
        "  \n",
        "  def get_mem_footprint(self):\n",
        "    fp = self.model.get_memory_footprint() * (10 ** -9)\n",
        "    return f\"memory footprint is approx {round(fp, 2)} GB\"\n",
        "\n",
        "  def summarize(self, long_text):\n",
        "    input_ids = self.tokenizer(long_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "    output = self.model.generate(input_ids, **params)\n",
        "    summary = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "FEwVNni1tzja"
      },
      "id": "FEwVNni1tzja",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = Summarizer()"
      ],
      "metadata": {
        "id": "SLa2onfBtsCX"
      },
      "id": "SLa2onfBtsCX",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer.get_mem_footprint()"
      ],
      "metadata": {
        "id": "oqdlDhL9v1Rz",
        "outputId": "449e70d2-8610-402a-dac6-938dc5793230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "oqdlDhL9v1Rz",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'memory footprint is approx 3.18 GB'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "long_text = \"\"\"\n",
        "A value that is outside the range of some numbers' global distribution is generally referred to as an outlier. Outlier detection has been widely used and covered in the current literature, and having prior knowledge of the distribution of your features helps with the task of outlier detection. More specifically, we have observed that classic quantization at scale fails for transformer-based models >6B parameters. While large outlier features are also present in smaller models, we observe that a certain threshold these outliers from highly systematic patterns across transformers which are present in every layer of the transformer. For more details on these phenomena see the LLM.int8() paper and emergent features blog post.\n",
        "\n",
        "As mentioned earlier, 8-bit precision is extremely constrained, therefore quantizing a vector with several big values can produce wildly erroneous results. Additionally, because of a built-in characteristic of the transformer-based architecture that links all the elements together, these errors tend to compound as they get propagated across multiple layers. Therefore, mixed-precision decomposition has been developed to facilitate efficient quantization with such extreme outliers. It is discussed next.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EpIn80o-ws3R"
      },
      "id": "EpIn80o-ws3R",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer.summarize(long_text)"
      ],
      "metadata": {
        "id": "eAtbPoGfxJXE",
        "outputId": "025b4bb1-7c90-4ac1-8a98-4f5c651f9cbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "eAtbPoGfxJXE",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Outliers are values that fall outside of the normal distribution. These are usually large values, such as the x and y axes.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}